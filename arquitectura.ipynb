{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura del ``Autoencoder``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from keras import Sequential\n",
    "from keras import losses, metrics, optimizers\n",
    "from keras.applications import mobilenet\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, LeakyReLU, MaxPooling2D, UpSampling2D, GlobalAveragePooling2D, Reshape\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regla de la piramide geometrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla de la piramide geometrica sirve para ayudar determinar el tamaño de las capas ocultas en base al tamaño de las capas de input y output y al numero de capas ocultas. \n",
    "\n",
    "Esta aproximacion esta propuesta por Masters(1993): \n",
    ">  \"For a three layer network with n input and m output neurons, the hidden layer would have sqrt(N * M) neurons.\"\n",
    ">\n",
    "> -- <cite> Masters, Timothy. Pratical neural network recipes in C++. Morgan Kaufmann, 1993.</cite>\n",
    "\n",
    "[Enlace al artículo](https://eulertech.wordpress.com/2018/01/02/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-network/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_rule(h_layers, input_size, output_size):\n",
    "    layers = []\n",
    "    if h_layers < 1:\n",
    "        print(\"No layers\")\n",
    "        return []\n",
    "    print(\"Layers for input %d and output %d:\" % (input_size,  output_size))\n",
    "    rate = (input_size/output_size)**(1/(h_layers+1))\n",
    "    for l in range(h_layers):\n",
    "        layer_size = output_size*(rate**(h_layers-l))\n",
    "        layer_size = round(layer_size)\n",
    "        layers.append(layer_size)\n",
    "        print(\"Layer %d: %d neurons\" % (l+1, layer_size))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMD(Earth Mover's Distance) ``WIP``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMD es una medida de distancia entre distribuciones de probabilidad, que consiste en representar ambas distribuciones como montones de tierra, en los que la distancia se determina en cuanto es el trabajo minimo que llevaria transformar un monticulo en otro. Matematicamente a EMD se la conoce como la métrica de Wasserstein.\n",
    "\n",
    "$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(p_i-q_i)^2}$\n",
    "\n",
    "Siendo p y q la prediccion y el verdadero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMD_loss(y_true, y_pred):\n",
    "\tn = np.prod(y_true.shape)\n",
    "\tp = tf.math.subtract(y_true, y_pred)\n",
    "\tp = tf.math.square(p)\n",
    "\tp = tf.math.reduce_sum(p)\n",
    "\treturn tf.math.sqrt(tf.math.divide(p,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Éste ``encoder`` va a ser un modelo basado en ``transfer learning``, vamos a tomar la red de ``mobilenet``, entranada para imagenes de ``imagenet`` sin las capa de clasificacion final, con una entrada de tamaño _img\\_shape_ y en el output colocamos la 'representacion latente' una codificacion de la imagen que nos permitiría reconstruirla con un ``decoder``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/preprocess_input\n",
    "# https://keras.io/guides/transfer_learning/\n",
    "def build_transf_encoder(dim_latente, img_shape, trainable=True):\n",
    "    inputs = Input(shape=img_shape)\n",
    "    x=tf.cast(inputs, tf.float32)\n",
    "    x=tf.keras.layers.Resizing(128,128)(x)\n",
    "    #x=keras.applications.mobilenet.preprocess_input(x)  #dataformat por defecto es chanel last\n",
    "    core = mobilenet.MobileNet(input_shape=((128,128,3)), weights=\"imagenet\", include_top=False)\n",
    "    core.trainable = trainable\n",
    "    model = core(x, training=trainable)\n",
    "    model = GlobalAveragePooling2D()(model)\n",
    "    repr_latente = Dense(dim_latente)(model)\n",
    "    return keras.Model(inputs, repr_latente)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_128_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 1s 0us/step\n",
      "17235968/17225924 [==============================] - 1s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " tf.cast (TFOpLambda)        (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " mobilenet_1.00_128 (Functio  (None, 4, 4, 1024)       3228864   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                20500     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,249,364\n",
      "Trainable params: 3,227,476\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = build_transf_encoder(20, (32,32,3))\n",
    "enc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder denso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_encoder(dim_latente, img_shape, depth=2):\n",
    "    layer_sizes = pyramid_rule(depth, np.prod(img_shape), dim_latente)\n",
    "    model =  keras.Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    for ls in layer_sizes:\n",
    "        model.add(Dense(ls))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(dim_latente))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers for input 3072 and output 20:\n",
      "Layer 1: 574 neurons\n",
      "Layer 2: 107 neurons\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 574)               1763902   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 574)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 107)               61525     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 107)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                2160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,827,587\n",
      "Trainable params: 1,827,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc = build_dense_encoder(20, (32,32,3))\n",
    "enc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_encoder(dim_latente, img_shape, depth=2):\n",
    "    model =  keras.Sequential()\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", activation=\"relu\", input_shape=img_shape))\n",
    "    model.add(MaxPooling2D())\n",
    "    for _ in range(depth-1):\n",
    "        model.add(Conv2D(32, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dim_latente))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                40980     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,124\n",
      "Trainable params: 51,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc = build_conv_encoder(20, (32,32,3))\n",
    "enc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder denso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dense_decoder(dim_latente, img_shape, depth=2):\n",
    "    model = keras.Sequential()\n",
    "    layer_sizes = pyramid_rule(depth, dim_latente, np.prod(img_shape))\n",
    "    model.add(Dense(layer_sizes[0], input_dim=dim_latente))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    for i in range(1, depth):\n",
    "        model.add(Dense(layer_sizes[i], input_dim=dim_latente))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(np.prod(img_shape), activation=keras.activations.sigmoid))\n",
    "    model.add(Reshape(img_shape))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers for input 20 and output 3072:\n",
      "Layer 1: 107 neurons\n",
      "Layer 2: 574 neurons\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 107)               2247      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 107)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 574)               61992     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 574)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3072)              1766400   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,830,639\n",
      "Trainable params: 1,830,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc = build_dense_decoder(20, (32,32,3))\n",
    "enc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_decoder(dim_latente, img_shape, depth=2):\n",
    "    model=keras.Sequential()\n",
    "    model.add(Dense(dim_latente, input_dim=(dim_latente)))\n",
    "    model.add(Dense(2048))              #hard coded\n",
    "    model.add(Reshape((8, 8, 32)))      #hard coded\n",
    "    for _ in range(depth):\n",
    "        model.add(Conv2D(32, (3,3), padding=\"same\"))\n",
    "        model.add(UpSampling2D())\n",
    "    model.add(Conv2D(3, (3, 3), padding=\"same\", activation=keras.activations.sigmoid))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2048)              43008     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 3)         867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,791\n",
      "Trainable params: 62,791\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec = build_conv_decoder(20, (32,32,3))\n",
    "dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = keras.optimizers.Adam(0.0002, 0.5)\n",
    "#dim_latente=64\n",
    "#img_shape=X_train[0].shape\n",
    "\n",
    "#encoder = build_conv_encoder(dim_latente, img_shape)\n",
    "#decoder = build_conv_decoder(dim_latente, img_shape)\n",
    "#img = keras.layers.Input(img_shape)\n",
    "\n",
    "#encoder_rep = encoder(img)\n",
    "#autoencoder_out = decoder(encoder_rep)\n",
    "\n",
    "#autoencoder = keras.Model(img, autoencoder_out)\n",
    "#autoencoder.compile(loss='mse', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "#out = autoencoder.predict(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El discriminador va a tener como entrada la codificacion latente de las imagenes y como salida una neurona que discrimina entre imagenes \"reales\" y \"falsas\". De esta forma entrenamos al encoder para que codifique con la distribucion que usemos para generar las \"imagenes reales\", en este caso, una distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(dim_latente, depth = 2):\n",
    "    layer_sizes = pyramid_rule(depth, dim_latente, 1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_sizes[0], input_dim=dim_latente))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    for i in range(1, depth):\n",
    "        model.add(Dense(layer_sizes[i]))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation=keras.activations.sigmoid))\n",
    "    encoded = Input(shape=dim_latente)\n",
    "    valid = model(encoded)\n",
    "    return keras.Model(encoded, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo para la construccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_AAE(dim_latente, img_shape, enc_model=build_dense_encoder, dec_model=build_dense_decoder, disc_model = build_discriminator, \n",
    "                 compilation_kwargs={}, enc_kwargs={}, dec_kwargs={}, disc_kwargs={}):\n",
    "    #Parameters\n",
    "    disc_params = {\"dim_latente\":dim_latente}\n",
    "    disc_params.update(disc_kwargs)\n",
    "    enc_params = {\"dim_latente\":dim_latente, \"img_shape\":img_shape}\n",
    "    enc_params.update(enc_kwargs)\n",
    "    dec_params = {\"dim_latente\":dim_latente, \"img_shape\":img_shape}\n",
    "    dec_params.update(dec_kwargs)\n",
    "\n",
    "    cp = {\"ae_loss\":losses.mean_squared_error, \n",
    "          \"disc_loss\": losses.binary_crossentropy, \n",
    "          \"optimizer\" : keras.optimizers.Adam(0.0002, 0.5)\n",
    "         } #cp = compilation params\n",
    "    cp.update(compilation_kwargs)\n",
    "    # Discriminador\n",
    "    discriminator = disc_model(**disc_params)\n",
    "    discriminator.compile(loss=cp[\"disc_loss\"], metrics=\"accuracy\")\n",
    "\n",
    "    # Encoder y decoder\n",
    "    encoder = enc_model(**enc_params)\n",
    "    decoder = dec_model(**dec_params)\n",
    "\n",
    "    # Autoencoder\n",
    "    # el encoder toma un imagen y la codifica y el decoder toma la codificacion e intenta regenerar la imagen\n",
    "    img = Input(shape=img_shape)\n",
    "    encoded = encoder(img)\n",
    "    reconstructed = decoder(encoded)\n",
    "\n",
    "    # para el autoencoder adversario solo queremos entrenar el generador, no el discriminador\n",
    "    discriminator.trainable=False\n",
    "\n",
    "    # El discriminador evalua la validez de la codificacion\n",
    "    validez = discriminator(encoded)\n",
    "\n",
    "    # Autoencoder adversario \n",
    "    a_autoencoder = keras.Model(img, [reconstructed, validez])\n",
    "    a_autoencoder.compile(loss=[cp[\"ae_loss\"], cp[\"disc_loss\"]], loss_weights=[0.999, 0.001], optimizer=cp[\"optimizer\"])\n",
    "    return (encoder, decoder, discriminator, a_autoencoder)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20fa4f0f3884cdb253ada201871cf8c3f8a990e4bbc1c7fce1b9b4c9c33df28e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
